{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UW-ERSL/TOuNN/blob/main/TOuNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgx2g5wK57ON"
      },
      "outputs": [],
      "source": [
        "# # run this first time to clone the directory \n",
        "# !git clone https://github.com/UW-ERSL/TOuNN.git\n",
        "# %cd TOuNN/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjvodDqPKJgB"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap4snKSi669D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "# from TOuNNOptimizer import TopologyOptimizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from os import path\n",
        "\n",
        "from tounn.FE import FE\n",
        "from tounn.plotUtil import Plotter\n",
        "from tounn.network import TopNet\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSIlH-z274SZ"
      },
      "source": [
        "### Mesh "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXlJJnLY7RGe"
      },
      "outputs": [],
      "source": [
        "nelx = 121 #61; \n",
        "nely = 61 #31; \n",
        "elemSize = np.array([1.0, 1.0]);\n",
        "mesh = {'nelx':nelx, 'nely':nely, 'elemSize':elemSize};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGqmMjQJKuFB"
      },
      "source": [
        "### Material"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUjLdr9SKUqK"
      },
      "outputs": [],
      "source": [
        "matProp = {'E':1.0, 'nu':0.3}; # Structural\n",
        "matProp['penal'] = 3.; # SIMP penalization constant, starting value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPbMfs-N7VZ5"
      },
      "source": [
        "### Boundary Condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z5LzQNg6sc5"
      },
      "outputs": [],
      "source": [
        "exampleName = 'TipCantilever'\n",
        "physics = 'Structural'\n",
        "ndof = 2*(nelx+1)*(nely+1);\n",
        "force = np.zeros((ndof,1))\n",
        "dofs=np.arange(ndof);\n",
        "fixed = dofs[0:2*(nely+1):1];\n",
        "force[2*(nelx+1)*(nely+1)-2*nely+1, 0 ] = -1;\n",
        "symXAxis = {'isOn':False, 'midPt':0.5*nely};\n",
        "symYAxis = {'isOn':False, 'midPt':0.5*nelx};\n",
        "bc = {'exampleName':exampleName, 'physics':physics, \\\n",
        "      'force':force, 'fixed':fixed, 'symXAxis':symXAxis, 'symYAxis':symYAxis };\n",
        "\n",
        "# For more BCs see examples.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poYB15u97tft"
      },
      "source": [
        "### NN Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6Apz8SV7sVy"
      },
      "outputs": [],
      "source": [
        "nnSettings = {'numLayers':3, 'numNeuronsPerLyr':20}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HF5v5E-K14v"
      },
      "source": [
        "### Constraints and Projections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhjAaWFhK0Bw"
      },
      "outputs": [],
      "source": [
        "densityProjection = {'isOn':True, 'sharpness':8};\n",
        "desiredVolumeFraction = 0.3;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJmPXas2CgZI"
      },
      "source": [
        "### Optimizer settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x18-MUuDCdW3"
      },
      "outputs": [],
      "source": [
        "minEpochs = 150; # minimum number of iterations\n",
        "maxEpochs = 5000; # Max number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TopologyOptimizer:\n",
        "    def __init__(self, mesh, matProp, bc, nnSettings, \\\n",
        "                  desiredVolumeFraction, densityProjection, overrideGPU = True):\n",
        "\n",
        "        self.exampleName = bc['exampleName'];\n",
        "        self.device = self.setDevice(overrideGPU);\n",
        "        self.boundaryResolution  = 3; # default value for plotting and interpreting\n",
        "        self.FE = FE(mesh, matProp, bc);\n",
        "        self.xy = torch.tensor(self.FE.elemCenters, requires_grad = True).\\\n",
        "                                        float().view(-1,2).to(self.device);\n",
        "        self.xyPlot = torch.tensor(self.FE.generatePoints(self.boundaryResolution, True),\\\n",
        "                        requires_grad = True).float().view(-1,2).to(self.device);\n",
        "        self.Pltr = Plotter();\n",
        "\n",
        "        self.desiredVolumeFraction = desiredVolumeFraction;\n",
        "        self.density = self.desiredVolumeFraction*np.ones((self.FE.numElems));\n",
        "        self.symXAxis = bc['symXAxis'];\n",
        "        self.symYAxis = bc['symYAxis'];\n",
        "\n",
        "        self.densityProjection = densityProjection;\n",
        "\n",
        "        inputDim = 2; # x and y coordn\n",
        "        self.topNet = TopNet(nnSettings, inputDim).to(self.device);\n",
        "        self.objective = 0.;\n",
        "\n",
        "    def setDevice(self, overrideGPU):\n",
        "        if(torch.cuda.is_available() and (overrideGPU == False) ):\n",
        "            device = torch.device(\"cuda:0\");\n",
        "            print(\"GPU enabled\")\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            print(\"Running on CPU\")\n",
        "        return device;\n",
        "\n",
        "    def applySymmetry(self, x):\n",
        "        if(self.symYAxis['isOn']):\n",
        "            xv =( self.symYAxis['midPt'] + torch.abs( x[:,0] - self.symYAxis['midPt']));\n",
        "        else:\n",
        "            xv = x[:,0];\n",
        "        if(self.symXAxis['isOn']):\n",
        "            yv = (self.symXAxis['midPt'] + torch.abs( x[:,1] - self.symXAxis['midPt'])) ;\n",
        "        else:\n",
        "            yv = x[:,1];\n",
        "        x = torch.transpose(torch.stack((xv,yv)),0,1);\n",
        "        return x;\n",
        "\n",
        "    def projectDensity(self, x):\n",
        "        if(self.densityProjection['isOn']):\n",
        "            b = self.densityProjection['sharpness']\n",
        "            nmr = np.tanh(0.5*b) + torch.tanh(b*(x-0.5));\n",
        "            x = 0.5*nmr/np.tanh(0.5*b);\n",
        "        return x;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "class pixelModel(nn.Module):\n",
        "    def __init__(self, nely=nely, nelx=nelx):\n",
        "        super().__init__()\n",
        "        self.nelx = nelx\n",
        "        self.nely = nely\n",
        "        self.param = nn.Parameter(torch.zeros(size=(nely, nelx), requires_grad=True))\n",
        "        \n",
        "    def forward(self):\n",
        "        a = self.param.sigmoid()\n",
        "        return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision\n",
        "gauss = torchvision.transforms.GaussianBlur(3, sigma=(0.5, .5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plt.imshow(rho_np.reshape(nely, nelx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = pixelModel()\n",
        "\n",
        "matProp = {'E':1.0, 'nu':0.3}; # Structural\n",
        "matProp['penal'] = 1.5; # SIMP penalization constant, starting value\n",
        "\n",
        "plt.close('all');\n",
        "overrideGPU = False\n",
        "start = time.perf_counter()\n",
        "topOpt = TopologyOptimizer(mesh, matProp, bc, nnSettings, \\\n",
        "                  desiredVolumeFraction, densityProjection, overrideGPU);\n",
        "\n",
        "topOpt.convergenceHistory = {'compliance':[], 'vol':[], 'grayElems':[]};\n",
        "learningRate = 1e-1;\n",
        "alphaMax = 100*topOpt.desiredVolumeFraction;\n",
        "alphaIncrement = 0.05;\n",
        "# alpha = alphaIncrement; # start\n",
        "alpha = 0; # start\n",
        "nrmThreshold = 0.05; # for gradient clipping\n",
        "# topOpt.optimizer = optim.Adam(topOpt.topNet.parameters(), amsgrad=True, lr=learningRate);\n",
        "topOpt.optimizer = optim.Adam(model.parameters(), amsgrad=True, lr=learningRate);\n",
        "\n",
        "\n",
        "topOpt.obj0 = 385.11016982915766\n",
        "# if(epoch == 0):\n",
        "#     topOpt.obj0 = ( topOpt.FE.Emax*(rho_np**topOpt.FE.penal)*Jelem).sum()\n",
        "# For sensitivity analysis, exponentiate by 2p here and divide by p in the loss func hence getting -ve sign\n",
        "\n",
        "# nn_rho = torch.nn.Parameter(torch.zeros(size=(nelx, nely), requires_grad=True)).flatten()\n",
        "\n",
        "for epoch in range(maxEpochs):\n",
        "    topOpt.optimizer.zero_grad();\n",
        "\n",
        "    # Parameters -> rho\n",
        "    # x = topOpt.applySymmetry(topOpt.xy);\n",
        "    # nn_rho = torch.flatten(topOpt.topNet(x)).to(topOpt.device);\n",
        "    # nn_rho = topOpt.projectDensity(nn_rho);\n",
        "    nn_rho = model().flatten()\n",
        "    # print(nn_rho.shape)\n",
        "\n",
        "    # Do the FEM-related computing\n",
        "    rho_np = nn_rho.cpu().detach().numpy(); # move tensor to numpy array   \n",
        "    topOpt.density = rho_np;\n",
        "    u, Jelem = topOpt.FE.solve(rho_np); # Call FE 88 line code [Niels Aage 2013]\n",
        "    \n",
        "    Jelem = np.array(topOpt.FE.Emax*(rho_np**(2*topOpt.FE.penal))*Jelem).reshape(-1);\n",
        "    Jelem = torch.tensor(Jelem).view(-1).float().to(topOpt.device);\n",
        "    objective = torch.sum( Jelem / (nn_rho**topOpt.FE.penal))/topOpt.obj0; # compliance\n",
        "\n",
        "    volConstraint = ((torch.mean(nn_rho)/topOpt.desiredVolumeFraction) - 1.0); # global vol constraint\n",
        "    currentVolumeFraction = np.average(rho_np);\n",
        "    topOpt.objective = objective;\n",
        "    loss = topOpt.objective + alpha * torch.pow(volConstraint, 2);\n",
        "\n",
        "    alpha = min(alphaMax, alpha + alphaIncrement);\n",
        "    loss.backward(retain_graph=True);\n",
        "    # torch.nn.utils.clip_grad_norm_(topOpt.topNet.parameters(), nrmThreshold)\n",
        "    topOpt.optimizer.step();\n",
        "\n",
        "    greyElements = sum(1 for rho in rho_np if ((rho > 0.05) & (rho < 0.95)));\n",
        "    relGreyElements = topOpt.desiredVolumeFraction * greyElements / len(rho_np);\n",
        "    topOpt.convergenceHistory['compliance'].append(topOpt.objective.item());\n",
        "    topOpt.convergenceHistory['vol'].append(currentVolumeFraction);\n",
        "    topOpt.convergenceHistory['grayElems'].append(relGreyElements);\n",
        "    # topOpt.FE.penal = min(8.0,topOpt.FE.penal + 0.02); # continuation scheme\n",
        "\n",
        "    if(epoch % 20 == 0):\n",
        "        titleStr = \"Iter {:d} , Obj {:.2F} , vol {:.2F}\".format(epoch, topOpt.objective.item()*topOpt.obj0, currentVolumeFraction);\n",
        "        topOpt.Pltr.plotDensity(topOpt.xy.detach().cpu().numpy(), rho_np.reshape((topOpt.FE.nelx, topOpt.FE.nely)), titleStr);\n",
        "        print(titleStr);\n",
        "    if ((epoch > minEpochs ) & (relGreyElements < 0.01)):\n",
        "        break;\n",
        "\n",
        "# topOpt.Pltr.plotDensity(topOpt.xy.detach().cpu().numpy(), rho_np.reshape((topOpt.FE.nelx, topOpt.FE.nely)), titleStr);\n",
        "# print(\"{:3d} J: {:.2F}; Vf: {:.3F}; loss: {:.3F}; relGreyElems: {:.3F} \"\\\n",
        "#         .format(epoch, topOpt.objective.item()*topOpt.obj0, currentVolumeFraction, loss.item(), relGreyElements));\n",
        "\n",
        "# print(\"Final J : {:.3f}\".format(topOpt.objective.item()*topOpt.obj0));\n",
        "# topOpt.Pltr.plotConvergence(topOpt.convergenceHistory);\n",
        "\n",
        "# x = topOpt.applySymmetry(topOpt.xyPlot);\n",
        "# rho = torch.flatten(topOpt.projectDensity(topOpt.topNet(x)));\n",
        "# rho_np = rho.cpu().detach().numpy();\n",
        "\n",
        "# titleStr = \"Iter {:d} , Obj {:.2F} , vol {:.2F}\".format(epoch, topOpt.objective.item()*topOpt.obj0, currentVolumeFraction);\n",
        "# topOpt.Pltr.plotDensity(topOpt.xyPlot.detach().cpu().numpy(), rho_np.reshape((topOpt.FE.nelx*topOpt.boundaryResolution, topOpt.FE.nely*topOpt.boundaryResolution)), titleStr);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x = topOpt.applySymmetry(topOpt.xy);\n",
        "# nn_rho = torch.flatten(topOpt.topNet(x)).to(topOpt.device);\n",
        "# nn_rho = topOpt.projectDensity(nn_rho);\n",
        "\n",
        "# # Do the FEM-related computing\n",
        "# rho_np = nn_rho.cpu().detach().numpy(); # move tensor to numpy array   \n",
        "# plt.imshow(rho_np.reshape((topOpt.FE.nelx, topOpt.FE.nely)).T)\n",
        "# plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "TOuNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
